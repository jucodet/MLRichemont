{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire la base de données\n",
    "import pandas as pd\n",
    "clothes=pd.read_csv('../data/wish/summer-products-with-rating-and-performance_2020-08.csv')\n",
    "\n",
    "clothes\n",
    "type(clothes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataviz\n",
    "import matplotlib.pyplot as plt\n",
    "prix_clothes=[1,3,5,7,9]\n",
    "prix_clothes_other=[2,4,6,8,10]\n",
    "annee=[2016,2017,2018,2019,2020]\n",
    "\n",
    "#Diagramme courbe\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(annee,prix_clothes,label='Prix fringues')\n",
    "plt.title('Diagramme courbe')\n",
    "\n",
    "#Diagramme nuage\n",
    "plt.subplot(2,2,2)\n",
    "plt.scatter(annee,prix_clothes_other, label='Prix fringues other',c='r')\n",
    "plt.legend()\n",
    "plt.xlabel('Année')\n",
    "plt.ylabel('Prix des fringues')\n",
    "plt.title('Diagramme nuage')\n",
    "\n",
    "#Diagramme bandes\n",
    "plt.bar(annee,prix_clothes_other, label='Prix fringues other')\n",
    "plt.bar(annee,prix_clothes,label='Prix fringues', bottom=prix_clothes_other)\n",
    "plt.legend()\n",
    "plt.xlabel('Année')\n",
    "plt.ylabel('Prix des fringues')\n",
    "plt.title('Diagramme de bandes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Chart\n",
    "import matplotlib.pyplot as plt\n",
    "produit=['P1','P2','P3','P4']\n",
    "vente=[10,20,20,50]\n",
    "plt.pie(vente,labels=produit, autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataviz\n",
    "import matplotlib.pyplot as plt\n",
    "prix_clothes=[1,3,5,7,9]\n",
    "prix_clothes_other=[2,4,6,8,10]\n",
    "annee=[2016,2017,2018,2019,2020]\n",
    "\n",
    "#Diagramme courbe\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(annee,prix_clothes,label='Prix fringues')\n",
    "plt.title('Diagramme courbe')\n",
    "\n",
    "#Diagramme nuage\n",
    "plt.subplot(2,2,2)\n",
    "plt.scatter(annee,prix_clothes_other, label='Prix fringues other',c='r')\n",
    "plt.legend()\n",
    "plt.xlabel('Année')\n",
    "plt.ylabel('Prix des fringues')\n",
    "plt.title('Diagramme nuage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataviz\n",
    "import matplotlib.pyplot as plt\n",
    "prix_clothes=[1,3,5,7,9]\n",
    "prix_clothes_other=[2,4,6,8,10]\n",
    "annee=[2016,2017,2018,2019,2020]\n",
    "\n",
    "#Diagramme nuage\n",
    "plt.subplot(2,2,2)\n",
    "plt.scatter(annee,prix_clothes_other, label='Prix fringues other',c='r')\n",
    "plt.legend()\n",
    "plt.xlabel('Année')\n",
    "plt.ylabel('Prix des fringues')\n",
    "plt.title('Diagramme nuage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataviz\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "prix_clothes=[1,3,5,7,9]\n",
    "prix_clothes_other=[2,4,6,8,10]\n",
    "annee=[2016,2017,2018,2019,2020]\n",
    "\n",
    "#Diagramme bandes\n",
    "index_annee=np.arange(len(annee))\n",
    "w=0.25\n",
    "\n",
    "plt.bar(index_annee,prix_clothes_other, label='Prix fringues other', width=w)\n",
    "plt.bar(index_annee+w,prix_clothes,label='Prix fringues', width=w)\n",
    "plt.legend()\n",
    "plt.xlabel('Année')\n",
    "plt.ylabel('Prix des fringues')\n",
    "plt.title('Diagramme de bandes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GoogleNewsSentimentAnalysis\n",
    "from pygooglenews import GoogleNews\n",
    "from textblob import TextBlob \n",
    "import re\n",
    "\n",
    "#gn.top_news(proxies = {'http':'160.202.145.206:80'})\n",
    "            \n",
    "#Montres LVMH : CHAUMET, TAG HEUER, ZENITH, BVLGARI, FRED, HUBLOT, TIFFANY & CO, LVMH\n",
    "#requête : montre (zenith or chaumet or \"tag heuer\" or bvlgari or fred or hublot or \"tiffany & co\"))\n",
    "#URL : https://news.google.com/u/1/search?q=montre%20(zenith%20or%20chaumet%20or%20%22tag%20heuer%22%20or%20bvlgari%20or%20fred%20or%20hublot%20or%20%22tiffany%20%26%20co%22))&hl=fr&gl=FR&ceid=FR%3Afr\n",
    "\n",
    "#business = gn.topic_headlines('business', proxies = {'http':'160.202.145.206:80'})\n",
    "#business = gn.topic_headlines('business', proxies = None, scraping_bee = \"GLJD8KXP7FPSJWDJWEFU087LFOPC8OPHM8V70MM4UMOWBH7ZQOIHGXXHNA0XRSFDHD52LA01NUAT1GJ8\");\n",
    "\n",
    "#montres = gn.search(query=\"watch%20(zenith%20or%20chaumet%20or%20'tag%20heuer'%20or%20bulgari%20or%20fred%20or%20hublot%20or%20'tiffany%20%26%20co')\", \n",
    "#                    helper = False, when = None, from_ = None, to_ = None, proxies={'http':'160.202.145.206:80'}, scraping_bee=None)\n",
    "\n",
    "#Ponctuation cleaning function\n",
    "def nlp_pipeline(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    text = ' '.join(text.split())\n",
    "    text = re.sub(r\"[A-Za-z\\.]*[0-9]+[A-Za-z%°\\.]*\", \"\", text)\n",
    "    text = re.sub(r\"(\\s\\-\\s|-$)\", \"\", text)\n",
    "    text = re.sub(r\"[,\\!\\?\\%\\(\\)\\/\\\"]\", \"\", text)\n",
    "    text = re.sub(r\"\\&\\S*\\s\", \"\", text)\n",
    "    text = re.sub(r\"\\&\", \"\", text)\n",
    "    text = re.sub(r\"\\+\", \"\", text)\n",
    "    text = re.sub(r\"\\#\", \"\", text)\n",
    "    text = re.sub(r\"\\$\", \"\", text)\n",
    "    text = re.sub(r\"\\£\", \"\", text)\n",
    "    text = re.sub(r\"\\%\", \"\", text)\n",
    "    text = re.sub(r\"\\:\", \"\", text)\n",
    "    text = re.sub(r\"\\@\", \"\", text)\n",
    "    text = re.sub(r\"\\-\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "gn = GoogleNews(lang = 'en', country = 'US')\n",
    "watchesBrandsSet = {\"CHAUMET\", \"TAG HEUER\", \"ZENITH\", \"BULGARI\", \"FRED\", \"HUBLOT\", \"TIFFANY & CO\", \"LVMH\"};\n",
    "\n",
    "#Searching for every brand\n",
    "for brand in watchesBrandsSet :\n",
    "    newsResultsForBrand = gn.search(query=\"watch%20\"+brand, \n",
    "                  helper = False, when = None, from_ = None, to_ = None, proxies={'http':'160.202.145.206:80'}, scraping_bee=None)\n",
    "\n",
    "    i=0\n",
    "    globalSentiment = 0\n",
    "    #Analysing sentiment for each news\n",
    "    while i < len(newsResultsForBrand['entries']) :\n",
    "        #Cleaning ponctuation\n",
    "        cleanNews = nlp_pipeline(newsResultsForBrand['entries'][i]['title'])\n",
    "        #print(cleanNews)\n",
    "        analysis = TextBlob(cleanNews)\n",
    "        globalSentiment += analysis.sentiment.polarity\n",
    "        #print('Sentiment=',analysis.sentiment.polarity,montres['entries'][i]['title'])\n",
    "        i+=1\n",
    "        \n",
    "    print('Sentiment global :', round(globalSentiment, 3), brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TwitterSentimentAnalysis\n",
    "import re \n",
    "import tweepy \n",
    "from twitter import Twitter\n",
    "from tweepy import OAuthHandler \n",
    "from textblob import TextBlob \n",
    "\n",
    "class TwitterClient(object): \n",
    "    def __init__(self): \n",
    "        # keys and tokens from the Twitter Dev Console \n",
    "        consumer_key = 'BWCDSuuJaBZhnwrUUXYc5y8zE'\n",
    "        consumer_secret = 'eGjLJSSV9aYD1DH7ZsZSjPnAY2AplfEbYgcJOFg9DnYxyhCCOh'\n",
    "        access_token = '888428862589341696-9iFkjJbQW9Lcxe5uWZNm7H5QoWIp3My'\n",
    "        access_token_secret = '888428862589341696-9iFkjJbQW9Lcxe5uWZNm7H5QoWIp3My'\n",
    "\n",
    "    # attempt authentication \n",
    "    try: \n",
    "        # create OAuthHandler object \n",
    "        self.auth = OAuthHandler(consumer_key, consumer_secret) \n",
    "        # set access token and secret \n",
    "        self.auth.set_access_token(access_token, access_token_secret) \n",
    "        # create tweepy API object to fetch tweets \n",
    "        self.api = tweepy.Api(self.auth) \n",
    "    except: \n",
    "        print(\"Error: Authentication Failed\") \n",
    "\n",
    "    def clean_tweet(self, tweet): \n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) \n",
    "\n",
    "    def get_tweet_sentiment(self, tweet): \n",
    "      \n",
    "        # create TextBlob object of passed tweet text \n",
    "        analysis = TextBlob(self.clean_tweet(tweet)) \n",
    "        # set sentiment \n",
    "        if analysis.sentiment.polarity > 0: \n",
    "            return 'positive'\n",
    "        elif analysis.sentiment.polarity == 0: \n",
    "            return 'neutral'\n",
    "        else: \n",
    "            return 'negative'\n",
    "\n",
    "    def get_tweets(self, query, count = 10): \n",
    "        \n",
    "        # empty list to store parsed tweets \n",
    "        tweets = [] \n",
    "\n",
    "        try: \n",
    "            # call twitter api to fetch tweets \n",
    "            fetched_tweets = self.api.search(q = query, count = count) \n",
    "\n",
    "            # parsing tweets one by one \n",
    "            for tweet in fetched_tweets: \n",
    "                # empty dictionary to store required params of a tweet \n",
    "                parsed_tweet = {} \n",
    "\n",
    "                # saving text of tweet \n",
    "                parsed_tweet['text'] = tweet.text \n",
    "                # saving sentiment of tweet \n",
    "                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text) \n",
    "\n",
    "                # appending parsed tweet to tweets list \n",
    "                if tweet.retweet_count > 0: \n",
    "                    # if tweet has retweets, ensure that it is appended only once \n",
    "                    if parsed_tweet not in tweets: \n",
    "                        tweets.append(parsed_tweet)\n",
    "                else: \n",
    "                    tweets.append(parsed_tweet) \n",
    "\n",
    "            # return parsed tweets \n",
    "            return tweets \n",
    "\n",
    "        except tweepy.TweepError as e: \n",
    "            # print error (if any) \n",
    "            print(\"Error : \" + str(e)) \n",
    "\n",
    "def main(): \n",
    "    # creating object of TwitterClient Class \n",
    "    api = TwitterClient() \n",
    "    # calling function to get tweets \n",
    "    tweets = api.get_tweets(query = 'Trump', count = 200) \n",
    "\n",
    "    # picking positive tweets from tweets \n",
    "    ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive'] \n",
    "    # percentage of positive tweets \n",
    "    print(\"Positive tweets percentage: {} %\".format(100*len(ptweets)/len(tweets))) \n",
    "    # picking negative tweets from tweets \n",
    "    ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative'] \n",
    "    # percentage of negative tweets \n",
    "    print(\"Negative tweets percentage: {} %\".format(100*len(ntweets)/len(tweets))) \n",
    "    # percentage of neutral tweets \n",
    "    print(\"Neutral tweets percentage: {} % \".format(100*(len(tweets) -(len( ntweets )+len( ptweets)))/len(tweets))) \n",
    "\n",
    "    # printing first 5 positive tweets \n",
    "    print(\"\\n\\nPositive tweets:\") \n",
    "    for tweet in ptweets[:10]: \n",
    "        print(tweet['text']) \n",
    "\n",
    "    # printing first 5 negative tweets \n",
    "    print(\"\\n\\nNegative tweets:\") \n",
    "    for tweet in ntweets[:10]: \n",
    "        print(tweet['text']) \n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    # calling main function \n",
    "    main() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweeter analysis variant\n",
    "import os\n",
    "import tweepy as tw\n",
    "\n",
    "consumer_key = 'BWCDSuuJaBZhnwrUUXYc5y8zE'\n",
    "consumer_secret = 'eGjLJSSV9aYD1DH7ZsZSjPnAY2AplfEbYgcJOFg9DnYxyhCCOh'\n",
    "access_key = '888428862589341696-9iFkjJbQW9Lcxe5uWZNm7H5QoWIp3My'\n",
    "access_secret = '888428862589341696-9iFkjJbQW9Lcxe5uWZNm7H5QoWIp3My'\n",
    "\n",
    "# Authentification :\n",
    "\n",
    "try:\n",
    "    auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tw.API(auth, wait_on_rate_limit=True)\n",
    "except: \n",
    "        print(\"Error: Authentication Failed\")\n",
    "        \n",
    "requete = \"Covid-19 OR Covid OR Corona OR Pandémie OR épidémie OR Coronavirus OR virus\"\n",
    "\n",
    "#tweets = api.get_tweets(query = 'Trump', count = 200)\n",
    "tweets = api.home_timeline()\n",
    "\n",
    "#tweets = tw.Cursor(api.search,\n",
    "#                   q = requete,\n",
    "#                   lang = \"fr\",\n",
    "#                   since='2018-01-15').items(1000)\n",
    "\n",
    "all_tweets = [tweet.text for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'twint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a6cd2977dd8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#ParseTwitter - https://pypi.org/project/twint/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtwint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest_asyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'twint'"
     ]
    }
   ],
   "source": [
    "#ParseTwitter - https://pypi.org/project/twint/\n",
    "import twint\n",
    "import sys\n",
    "import nest_asyncio\n",
    "from pprint import pprint\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#Ponctuation cleaning function\n",
    "def nlp_pipeline(text):\n",
    "    print(\"dans le pipe\")\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    text = ' '.join(text.split())\n",
    "    text = re.sub(r\"[A-Za-z\\.]*[0-9]+[A-Za-z%°\\.]*\", \"\", text)\n",
    "    text = re.sub(r\"(\\s\\-\\s|-$)\", \"\", text)\n",
    "    text = re.sub(r\"[,\\!\\?\\%\\(\\)\\/\\\"]\", \"\", text)\n",
    "    text = re.sub(r\"\\&\\S*\\s\", \"\", text)\n",
    "    text = re.sub(r\"\\&\", \"\", text)\n",
    "    text = re.sub(r\"\\+\", \"\", text)\n",
    "    text = re.sub(r\"\\#\", \"\", text)\n",
    "    text = re.sub(r\"\\$\", \"\", text)\n",
    "    text = re.sub(r\"\\£\", \"\", text)\n",
    "    text = re.sub(r\"\\%\", \"\", text)\n",
    "    text = re.sub(r\"\\:\", \"\", text)\n",
    "    text = re.sub(r\"\\@\", \"\", text)\n",
    "    text = re.sub(r\"\\-\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "  \n",
    "    tw = twint.Config()\n",
    "    tw.Since = \"2020-02-01 12:00:00\"\n",
    "    tw.Custom[\"tweet\"] = [\"id\"]\n",
    "    tw.Pandas = True\n",
    "    tw.Lang = \"en\"\n",
    "\n",
    "    watchesBrandsSet = {\"CHAUMET\", \"TAG HEUER\", \"ZENITH\", \"BULGARI\", \"FRED\", \"HUBLOT\", \"TIFFANY & CO\", \"LVMH\"};\n",
    "\n",
    "    for brand in watchesBrandsSet :\n",
    "        # We open the log file in writting mode\n",
    "        with open('tweetsMarque', 'w') as f:\n",
    "            \n",
    "            # We redirect the 'sys.stdout' command towards the descriptor file\n",
    "            sys.stdout = f\n",
    "            print(\"toto\")\n",
    "            tw.Search = \"watch AND \"+brand\n",
    "            twint.run.Search(tw)\n",
    "            tweet = twint.storage.panda.Tweets_df\n",
    "            tweet = tweet.apply(nlp_pipeline)\n",
    "            #corpus = df['tweet']\n",
    "            #corpusClean = corpus.apply(nlp_pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
